{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Mining, Classification, and  Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usamanisar20/Python-projects/blob/main/Text_Mining%2C_Classification%2C_and_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYpT3gdPHfl0"
      },
      "source": [
        "**Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuvD3-D8cPXX"
      },
      "source": [
        "# Importing Libraries for developing a language model\n",
        "import nltk\n",
        "import textblob\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w38aNTDdDF7"
      },
      "source": [
        "**Question 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcJlmECFqhiq"
      },
      "source": [
        "import re #Importing regular expression module"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChVH9wDWqk0-"
      },
      "source": [
        "def N_Grams(text,n): \n",
        "    # spliting sentences into tokens\n",
        "    tokens=re.split(\"\\\\s+\",text)\n",
        "    ngrams=[] \n",
        "    # collecting the n-grams\n",
        "    for i in range(len(tokens)-n+1):\n",
        "     temp=[tokens[j] for j in range(i,i+n)]\n",
        "     ngrams.append(\" \".join(temp)) \n",
        "    return ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkIwOKdLLDGR"
      },
      "source": [
        "fileOpen = open(\"Musical_Instruments_Reviews.csv\")\n",
        "text1 = fileOpen.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ3aV5_cWQo-",
        "outputId": "14d22888-6e44-41c7-f2ce-2e4797496ad4"
      },
      "source": [
        "# Answer to question number 1 part c\n",
        "\n",
        "def count(token, x):\n",
        "    return token.count(x)\n",
        "\n",
        "fielopen = open('Text Corpus.txt', 'r')\n",
        "tempText = fielopen.read()\n",
        "\n",
        "text2 = tempText.replace('<s>', '')    #Removing sentence pads \n",
        "FinalText = text2.replace('</s>', '')  \n",
        "\n",
        "tokens = nltk.tokenize.word_tokenize(FinalText.lower())   #Tokenization of the sentences\n",
        "\n",
        "uniqueTokens=[]\n",
        "\n",
        "for i in tokens:\n",
        "    if i not in uniqueTokens:\n",
        "        uniqueTokens.append(i)\n",
        "\n",
        "print(\"-----------------------------\")\n",
        "print(\"--> Unsmoothed model <--\")\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "#Calulating word probability of the Unsmoothed model\n",
        "for i in range(len(uniqueTokens)):\n",
        "    print(\"Probability of \", uniqueTokens[i], \":\", round(count(tokens, uniqueTokens[i]) / len(tokens), 3)) \n",
        "\n",
        "#Calulating word probability of the Smoothed model\n",
        "print(\"\\n-----------------------------\")\n",
        "print(\"--> Smoothed model <--\")\n",
        "print(\"-----------------------------\")\n",
        "for i in range(len(uniqueTokens)):\n",
        "    print(\"Probability of \", uniqueTokens[i], \":\", round((count(tokens, uniqueTokens[i]) + 1) / (len(tokens) + (len(uniqueTokens)+1)), 3))\n",
        "\n",
        "fielopen.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "--> Unsmoothed model <--\n",
            "-----------------------------\n",
            "Probability of  he : 0.133\n",
            "Probability of  read : 0.2\n",
            "Probability of  a : 0.2\n",
            "Probability of  book : 0.2\n",
            "Probability of  i : 0.067\n",
            "Probability of  different : 0.067\n",
            "Probability of  my : 0.067\n",
            "Probability of  mulan : 0.067\n",
            "\n",
            "-----------------------------\n",
            "--> Smoothed model <--\n",
            "-----------------------------\n",
            "Probability of  he : 0.125\n",
            "Probability of  read : 0.167\n",
            "Probability of  a : 0.167\n",
            "Probability of  book : 0.167\n",
            "Probability of  i : 0.083\n",
            "Probability of  different : 0.083\n",
            "Probability of  my : 0.083\n",
            "Probability of  mulan : 0.083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvOpv1OPzi3u"
      },
      "source": [
        "**Question 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfIYm38lXakn",
        "outputId": "4fe81621-ce68-40a0-fa75-9518e752e8f4"
      },
      "source": [
        "# Answer to question number 2 part c\n",
        "#Bi-gram langauge model\n",
        "\n",
        "fielopen2 = open('Text Corpus.txt', 'r') #File operations\n",
        "\n",
        "Text2 = fielopen2.read()\n",
        "textTokenation=Text2.split()\n",
        "\n",
        "list2 = list(ngrams(textTokenation,n=2)) #Tokenization\n",
        "\n",
        "#Opeartions for calculating bi-gram probabilities for Unsmoothed model\n",
        "print(\"-----------------------------\")\n",
        "print(\"--> Unsmoothed model <--\")\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "for i in range(len(list2)):\n",
        "    if list2[i] != ('<s>', \"</s>\") and list2[i] != ('</s>', '<s>'):\n",
        "        print(\"P(\", list2[i][1], \"|\", list2[i][0], \")\", \"=\", list2.count(list2[i]), \"/\",\n",
        "              textTokenation.count(list2[i][0]), \"=\",\n",
        "              round(list2.count(list2[i]) / textTokenation.count(list2[i][0]), 3))\n",
        "\n",
        "#Opeartions for calculating bi-gram probabilities for Smoothed model\n",
        "\n",
        "print(\"\\n-----------------------------\")\n",
        "print(\"--> Smoothed model <--\")\n",
        "print(\"-----------------------------\")\n",
        "for i in range(len(list2)):\n",
        "    if list2[i]!=('<s>',\"</s>\") and list2[i] != ('</s>','<s>'):\n",
        "        print(\"P(\", list2[i][1], \"|\", list2[i][0], \")\", \"=\", list2.count(list2[i]) + 1, \"/\",\n",
        "              textTokenation.count(list2[i][0]) + 10 + 1, \"=\",\n",
        "              round((list2.count(list2[i]) + 1) / (textTokenation.count(list2[i][0]) + 10 + 1), 3))\n",
        "\n",
        "fielopen2.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "--> Unsmoothed model <--\n",
            "-----------------------------\n",
            "P( He | <s> ) = 2 / 3 = 0.667\n",
            "P( read | He ) = 2 / 2 = 1.0\n",
            "P( a | read ) = 3 / 3 = 1.0\n",
            "P( book | a ) = 2 / 3 = 0.667\n",
            "P( </s> | book ) = 2 / 3 = 0.667\n",
            "P( I | <s> ) = 1 / 3 = 0.333\n",
            "P( read | I ) = 1 / 1 = 1.0\n",
            "P( a | read ) = 3 / 3 = 1.0\n",
            "P( different | a ) = 1 / 3 = 0.333\n",
            "P( book | different ) = 1 / 1 = 1.0\n",
            "P( </s> | book ) = 2 / 3 = 0.667\n",
            "P( He | <s> ) = 2 / 3 = 0.667\n",
            "P( read | He ) = 2 / 2 = 1.0\n",
            "P( a | read ) = 3 / 3 = 1.0\n",
            "P( book | a ) = 2 / 3 = 0.667\n",
            "P( my | book ) = 1 / 3 = 0.333\n",
            "P( Mulan | my ) = 1 / 1 = 1.0\n",
            "P( </s> | Mulan ) = 1 / 1 = 1.0\n",
            "\n",
            "-----------------------------\n",
            "--> Smoothed model <--\n",
            "-----------------------------\n",
            "P( He | <s> ) = 3 / 14 = 0.214\n",
            "P( read | He ) = 3 / 13 = 0.231\n",
            "P( a | read ) = 4 / 14 = 0.286\n",
            "P( book | a ) = 3 / 14 = 0.214\n",
            "P( </s> | book ) = 3 / 14 = 0.214\n",
            "P( I | <s> ) = 2 / 14 = 0.143\n",
            "P( read | I ) = 2 / 12 = 0.167\n",
            "P( a | read ) = 4 / 14 = 0.286\n",
            "P( different | a ) = 2 / 14 = 0.143\n",
            "P( book | different ) = 2 / 12 = 0.167\n",
            "P( </s> | book ) = 3 / 14 = 0.214\n",
            "P( He | <s> ) = 3 / 14 = 0.214\n",
            "P( read | He ) = 3 / 13 = 0.231\n",
            "P( a | read ) = 4 / 14 = 0.286\n",
            "P( book | a ) = 3 / 14 = 0.214\n",
            "P( my | book ) = 2 / 14 = 0.143\n",
            "P( Mulan | my ) = 2 / 12 = 0.167\n",
            "P( </s> | Mulan ) = 2 / 12 = 0.167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV0rgkFV0bue"
      },
      "source": [
        "**QUESTION 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1nfhZ901Z1l"
      },
      "source": [
        "#TXSA Group Assignment - Q3 - Lee\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "\n",
        "def count(token, x):\n",
        "    return token.count(x)\n",
        "\n",
        "fileopen = open('Text Corpus.txt', 'r')\n",
        "tempText = fileopen.read()\n",
        "\n",
        "text2 = tempText.replace('<s>', '')    #Removing sentence pads\n",
        "FinalText = text2.replace('</s>', '')\n",
        "\n",
        "tokens = nltk.tokenize.word_tokenize(FinalText.lower())   #Tokenization of the sentences\n",
        "\n",
        "uniqueTokens=[]\n",
        "\n",
        "for i in tokens:\n",
        "    if i not in uniqueTokens:\n",
        "        uniqueTokens.append(i)\n",
        "\n",
        "#Sentence Probabilities: Smoothed Unigram\n",
        "textlist = [\"He read a book\",\"I read a different book \", \"He read a book my Mulan\"]\n",
        "\n",
        "print(\"Sentence probabilities by using smoothed unigram model:\")\n",
        "for m in range(len(textlist)):\n",
        "    targetTokens= nltk.tokenize.word_tokenize(textlist[m].lower())\n",
        "    a = 1\n",
        "    for i in range(len(targetTokens)):\n",
        "        b = round((count(tokens, targetTokens[i]) + 1) / (len(tokens) + (len(uniqueTokens)+1)), 3)\n",
        "        a = a * b\n",
        "    print(\"\\\"\",textlist[m],\"\\\"\",\": \", round(a,10))\n",
        "\n",
        "print()\n",
        "#Sentence Probaibilities: Smoothed Bigram\n",
        "textTokenation=tempText.split()\n",
        "list1 = list(ngrams(textTokenation,n=2)) #Tokenization\n",
        "\n",
        "textlist2 = [\"<s> He read a book </s>\",\"<s> I read a different book </s>\", \"<s> He read a book my Mulan </s>\"]\n",
        "\n",
        "print(\"Sentence probabilities by using smoothed bigram model:\")\n",
        "for m in range(len(textlist2)):\n",
        "    textTokenation2 = textlist2[m].split()\n",
        "    list2 = list(ngrams(textTokenation2,n=2)) #Tokenization\n",
        "    a = 1\n",
        "    for i in range(len(list2)):\n",
        "        b = round((list1.count(list2[i]) + 1) / (textTokenation.count(list2[i][0]) + 10 + 1), 3)\n",
        "        a = a * b\n",
        "    print(\"\\\"\",textlist2[m],\"\\\"\",\": \", round(a,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP7WelTMHmIt"
      },
      "source": [
        "**QUESTION 4**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpdM_tP8IAo9"
      },
      "source": [
        "Importing data set and \n",
        "libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9xmqCtHluo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbc790e-a4e3-453e-dfc4-f71a9faf7533"
      },
      "source": [
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "#Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk, keras, string, re, html, math\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "nltk.download(\"punkit\")\n",
        "\n",
        "import collections\n",
        "from nltk.metrics import*\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import NaiveBayesClassifier, classify"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkit: Package 'punkit' not found in index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAZpzgRzIvD4"
      },
      "source": [
        "data = pd.read_csv(r'/content/Musical_Instruments_Reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d55gaDZsJOL9",
        "outputId": "484dcd09-3f25-4d56-b958-6dcc8b2e4485"
      },
      "source": [
        " \n",
        "#assign column names\n",
        "def analysis(Polarity):\n",
        "    if Polarity < 0:\n",
        "        return 'Negative'\n",
        "    elif Polarity == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Positive'\n",
        "    \n",
        " #apply polarity function each row of review columnn\\n\",\n",
        "data[['polarity']] = data['Reviews'].apply(lambda x:TextBlob(x).polarity).to_list() \n",
        "data[['analysis']] = data['polarity'].apply(analysis).to_list() \n",
        "\n",
        "    \n",
        "columns = ['Reviews', 'Polarity','analysis']\n",
        "#print dataframe\\n\n",
        "\n",
        "print(data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 Reviews  polarity  analysis\n",
            "0      Not much to write about here, but it does exac...  0.250000  Positive\n",
            "1      The product does exactly as it should and is q...  0.052778  Positive\n",
            "2      The primary job of this device is to block the...  0.167500  Positive\n",
            "3      Nice windscreen protects my MXL mic and preven...  0.200000  Positive\n",
            "4      This pop filter is great. It looks and perform...  0.800000  Positive\n",
            "...                                                  ...       ...       ...\n",
            "10249            Great, just as expected.  Thank to all.  0.350000  Positive\n",
            "10250  I've been thinking about trying the Nanoweb st...  0.188033  Positive\n",
            "10251  I have tried coated strings in the past ( incl...  0.197768  Positive\n",
            "10252  Well, MADE by Elixir and DEVELOPED with Taylor...  0.153843  Positive\n",
            "10253  These strings are really quite good, but I wou...  0.335333  Positive\n",
            "\n",
            "[10254 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFRcKSBNirb_"
      },
      "source": [
        "Answer to question 4-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUvSaY60PrFT"
      },
      "source": [
        " # Exporting the predicted sentiments to ReviewsPolarity.csv\n",
        "\n",
        "data.to_csv(r'ReviewsPolarity.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W6x1xlWZ4PX"
      },
      "source": [
        "Question 4-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQGEaQbCRoBg",
        "outputId": "21760909-1fcd-47c1-bb9f-15bb931b385e"
      },
      "source": [
        "#Importing the new exported Dataset\n",
        "dataset = pd.read_csv(r'/content/ReviewsPolarity.csv')\n",
        "\n",
        "dataset.drop(['polarity'],axis = 1, inplace = True)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 Reviews  analysis\n",
            "0      Not much to write about here, but it does exac...  Positive\n",
            "1      The product does exactly as it should and is q...  Positive\n",
            "2      The primary job of this device is to block the...  Positive\n",
            "3      Nice windscreen protects my MXL mic and preven...  Positive\n",
            "4      This pop filter is great. It looks and perform...  Positive\n",
            "...                                                  ...       ...\n",
            "10249            Great, just as expected.  Thank to all.  Positive\n",
            "10250  I've been thinking about trying the Nanoweb st...  Positive\n",
            "10251  I have tried coated strings in the past ( incl...  Positive\n",
            "10252  Well, MADE by Elixir and DEVELOPED with Taylor...  Positive\n",
            "10253  These strings are really quite good, but I wou...  Positive\n",
            "\n",
            "[10254 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aRFX1bluyMG",
        "outputId": "0a175f27-d39d-4386-cb1a-428b042e6136"
      },
      "source": [
        "# Converting all values in each column to lower case\n",
        "\n",
        "dataset[\"Reviews\"]= dataset[\"Reviews\"].str.lower() \n",
        "dataset[\"analysis\"]= dataset[\"analysis\"].str.lower()\n",
        "dataset.head"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                  Reviews  analysis\n",
              "0      not much to write about here, but it does exac...  positive\n",
              "1      the product does exactly as it should and is q...  positive\n",
              "2      the primary job of this device is to block the...  positive\n",
              "3      nice windscreen protects my mxl mic and preven...  positive\n",
              "4      this pop filter is great. it looks and perform...  positive\n",
              "...                                                  ...       ...\n",
              "10249            great, just as expected.  thank to all.  positive\n",
              "10250  i've been thinking about trying the nanoweb st...  positive\n",
              "10251  i have tried coated strings in the past ( incl...  positive\n",
              "10252  well, made by elixir and developed with taylor...  positive\n",
              "10253  these strings are really quite good, but i wou...  positive\n",
              "\n",
              "[10254 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPzRwueDxcjm",
        "outputId": "ed0b47a0-04c8-49d7-9b89-603fa27e7a23"
      },
      "source": [
        "#-> Cleaning text data method <-\n",
        "\n",
        "def cleaning(dataset):\n",
        "  #removes any HTML tags\n",
        "    clean = re.sub('<.*?>', ' ', str(dataset))   \n",
        "\n",
        "#removes any hanging letters afer apostrophes (s in it's)\n",
        "    clean = re.sub('\\'.*?\\s',' ', clean)          \n",
        "\n",
        "#removes any URLs\n",
        "    clean = re.sub(r'http\\S+',' ', clean)      \n",
        "\n",
        "#replacing any non alphanumeric characters\n",
        "    clean = re.sub('\\W+',' ', clean)                   \n",
        "\n",
        "    return html.unescape(clean)\n",
        "\n",
        "    \n",
        "dataset['cleaned'] = dataset['Reviews'].apply(cleaning)  # cleaning text data\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 Reviews  ...                                            cleaned\n",
            "0      not much to write about here, but it does exac...  ...  not much to write about here but it does exact...\n",
            "1      the product does exactly as it should and is q...  ...  the product does exactly as it should and is q...\n",
            "2      the primary job of this device is to block the...  ...  the primary job of this device is to block the...\n",
            "3      nice windscreen protects my mxl mic and preven...  ...  nice windscreen protects my mxl mic and preven...\n",
            "4      this pop filter is great. it looks and perform...  ...  this pop filter is great it looks and performs...\n",
            "...                                                  ...  ...                                                ...\n",
            "10249            great, just as expected.  thank to all.  ...               great just as expected thank to all \n",
            "10250  i've been thinking about trying the nanoweb st...  ...  i been thinking about trying the nanoweb strin...\n",
            "10251  i have tried coated strings in the past ( incl...  ...  i have tried coated strings in the past includ...\n",
            "10252  well, made by elixir and developed with taylor...  ...  well made by elixir and developed with taylor ...\n",
            "10253  these strings are really quite good, but i wou...  ...  these strings are really quite good but i woul...\n",
            "\n",
            "[10254 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6zV_YNE4yXp",
        "outputId": "7c35f40a-1ddd-46ec-ffcf-7a26dd56b3b1"
      },
      "source": [
        "# Tokenization of text data\n",
        "nltk.download('punkt')\n",
        "def tokenizing(dataset):\n",
        "                             \n",
        "    tokens = nltk.word_tokenize(dataset['cleaned'] )\n",
        "    return tokens\n",
        "#tokenizing is done\n",
        "dataset['tokens'] = dataset.apply(tokenizing, axis=1) \n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "                                                 Reviews  ...                                             tokens\n",
            "0      not much to write about here, but it does exac...  ...  [not, much, to, write, about, here, but, it, d...\n",
            "1      the product does exactly as it should and is q...  ...  [the, product, does, exactly, as, it, should, ...\n",
            "2      the primary job of this device is to block the...  ...  [the, primary, job, of, this, device, is, to, ...\n",
            "3      nice windscreen protects my mxl mic and preven...  ...  [nice, windscreen, protects, my, mxl, mic, and...\n",
            "4      this pop filter is great. it looks and perform...  ...  [this, pop, filter, is, great, it, looks, and,...\n",
            "...                                                  ...  ...                                                ...\n",
            "10249            great, just as expected.  thank to all.  ...        [great, just, as, expected, thank, to, all]\n",
            "10250  i've been thinking about trying the nanoweb st...  ...  [i, been, thinking, about, trying, the, nanowe...\n",
            "10251  i have tried coated strings in the past ( incl...  ...  [i, have, tried, coated, strings, in, the, pas...\n",
            "10252  well, made by elixir and developed with taylor...  ...  [well, made, by, elixir, and, developed, with,...\n",
            "10253  these strings are really quite good, but i wou...  ...  [these, strings, are, really, quite, good, but...\n",
            "\n",
            "[10254 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtlsKcF39Nl1",
        "outputId": "ec5645bd-9c07-4d4d-a6f0-85ea6c6b1741"
      },
      "source": [
        " # Removing stopwords from the tokenized data\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stops(dataset):\n",
        "    my_list = dataset['tokens']\n",
        "    meaningful_words = [w for w in my_list if not w in stop_words]        \n",
        "    return (meaningful_words)\n",
        "dataset['tokens'] = dataset.apply(remove_stops, axis=1)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "                                                 Reviews  ...                                             tokens\n",
            "0      not much to write about here, but it does exac...  ...  [much, write, exactly, supposed, filters, pop,...\n",
            "1      the product does exactly as it should and is q...  ...  [product, exactly, quite, affordable, realized...\n",
            "2      the primary job of this device is to block the...  ...  [primary, job, device, block, breath, would, o...\n",
            "3      nice windscreen protects my mxl mic and preven...  ...  [nice, windscreen, protects, mxl, mic, prevent...\n",
            "4      this pop filter is great. it looks and perform...  ...  [pop, filter, great, looks, performs, like, st...\n",
            "...                                                  ...  ...                                                ...\n",
            "10249            great, just as expected.  thank to all.  ...                           [great, expected, thank]\n",
            "10250  i've been thinking about trying the nanoweb st...  ...  [thinking, trying, nanoweb, strings, bit, put,...\n",
            "10251  i have tried coated strings in the past ( incl...  ...  [tried, coated, strings, past, including, elix...\n",
            "10252  well, made by elixir and developed with taylor...  ...  [well, made, elixir, developed, taylor, guitar...\n",
            "10253  these strings are really quite good, but i wou...  ...  [strings, really, quite, good, call, perfect, ...\n",
            "\n",
            "[10254 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNC6LdXC9nvV",
        "outputId": "f2e6b4b0-5bd0-4188-e7c5-3fe2fcb731f7"
      },
      "source": [
        "# Lemmatizing is performed. As it is more efficient than stemming.\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatizing(dataset):\n",
        "    my_list = dataset['tokens']\n",
        "    lemmatized_list = [lemmatizer.lemmatize(word) for word in my_list]    \n",
        "    return (lemmatized_list)\n",
        "dataset['tokens'] = dataset.apply(lemmatizing, axis=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRv7x7cF99Zp",
        "outputId": "8f5d5a94-c024-47f6-a1c6-2a8428f06e85"
      },
      "source": [
        "# Rejoining all stemmed words\n",
        "def rejoin_words(dataset):\n",
        "    my_list = dataset['tokens']\n",
        "    joined_words = ( \" \".join(my_list))                     \n",
        "    return joined_words\n",
        "dataset['cleaned'] = dataset.apply(rejoin_words, axis=1)\n",
        "\n",
        "print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 Reviews  ...                                             tokens\n",
            "0      not much to write about here, but it does exac...  ...  [much, write, exactly, supposed, filter, pop, ...\n",
            "1      the product does exactly as it should and is q...  ...  [product, exactly, quite, affordable, realized...\n",
            "2      the primary job of this device is to block the...  ...  [primary, job, device, block, breath, would, o...\n",
            "3      nice windscreen protects my mxl mic and preven...  ...  [nice, windscreen, protects, mxl, mic, prevent...\n",
            "4      this pop filter is great. it looks and perform...  ...  [pop, filter, great, look, performs, like, stu...\n",
            "...                                                  ...  ...                                                ...\n",
            "10249            great, just as expected.  thank to all.  ...                           [great, expected, thank]\n",
            "10250  i've been thinking about trying the nanoweb st...  ...  [thinking, trying, nanoweb, string, bit, put, ...\n",
            "10251  i have tried coated strings in the past ( incl...  ...  [tried, coated, string, past, including, elixi...\n",
            "10252  well, made by elixir and developed with taylor...  ...  [well, made, elixir, developed, taylor, guitar...\n",
            "10253  these strings are really quite good, but i wou...  ...  [string, really, quite, good, call, perfect, u...\n",
            "\n",
            "[10254 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMNu27ZtRvoT",
        "outputId": "62be5040-7cd0-4f76-e882-1c46351ec846"
      },
      "source": [
        "# Model Building using NLTK with NaiveBayes Classifier (assignment Work)\n",
        "\n",
        "def feature_Sel(dataframe):\n",
        "  tokens = dataframe\n",
        "  return dict([(token,True) for token in tokens])\n",
        "\n",
        "dataV = [(feature_Sel(i[\"tokens\"]),i[\"analysis\"]) for _, i in dataset.iterrows()]\n",
        "\n",
        "# Dataset partitioninto 80/20 pattern\n",
        "random.shuffle(dataV)\n",
        "split=int(len(dataV)*0.8)\n",
        "train_Dataset=dataV[:split]\n",
        "test_Dataset=dataV[split:]\n",
        "\n",
        "# Model Building of Naive Bayes using NLTK\n",
        "model_classifier = NaiveBayesClassifier.train(train_Dataset)\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9068746952705997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS60P2I_WexY",
        "outputId": "11bbd84e-69a9-4040-ca23-fa420a3367b5"
      },
      "source": [
        "# First Method to find performances\n",
        "\n",
        "from nltk.metrics.scores import (\n",
        "    accuracy,\n",
        "    approxrand,\n",
        "    f_measure,\n",
        "    log_likelihood,\n",
        "    precision,\n",
        "    recall,\n",
        ")\n",
        "\n",
        "refsets = collections.defaultdict(set)\n",
        "testsets = collections.defaultdict(set)\n",
        "\n",
        "\n",
        "for i, (feats, label) in enumerate(test_Dataset):\n",
        "  refsets[label].add(i)\n",
        "  observed = model_classifier.classify(feats)\n",
        "  testsets[observed].add(i)\n",
        "print ('pos precision:', round((precision(refsets['positive'], testsets['positive'])),4))\n",
        "print('pos recall:', round((precision(refsets['positive'], testsets['positive'])),4))\n",
        "print ('pos F-measure:', round((f_measure(refsets['positive'], testsets['positive'])),4))\n",
        "print ('neg precision:', round((precision(refsets['negative'], testsets['negative'])),4))\n",
        "print ('neg recall:', round((recall(refsets['negative'], testsets['negative'])),4))\n",
        "print ('neg F-measure:', round((f_measure(refsets['negative'], testsets['negative'])),4))\n",
        "\n",
        "# Second  Method to find performances manually add\n",
        "True_Positive=0\n",
        "True_Negative=0\n",
        "False_Positive=0\n",
        "False_Negative=0\n",
        "\n",
        "calculation_errors =[]\n",
        "\n",
        "Listing_Predictions=[]\n",
        "\n",
        "\n",
        "for (feats , label) in test_Dataset:\n",
        "  check = model_classifier.classify(feats)\n",
        "  Listing_Predictions.append((label,check,feats))\n",
        "  if check == \"positive\":\n",
        "    if label == \"positive\":\n",
        "      True_Positive +=1\n",
        "    else:\n",
        "      False_Positive +=1\n",
        "  else:\n",
        "    if label==\"negative\":\n",
        "      True_Negative +=1\n",
        "    else:\n",
        "      False_Negative +=1\n",
        "  if check != label:\n",
        "    calculation_errors.append((label,check,feats))\n",
        "\n",
        "print(\"----------------------------\")\n",
        "print(\"    Classification Table\")\n",
        "print(\"----------------------------\")\n",
        "print(\"True Positive (TP) = \",True_Positive)\n",
        "print(\"True Negative (TN) = \",True_Negative)\n",
        "print(\"False Positive (FP) = \", False_Positive)\n",
        "print(\"False Negative (FN) = \", False_Negative)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos precision: 0.9103\n",
            "pos recall: 0.9103\n",
            "pos F-measure: 0.9523\n",
            "neg precision: 0.5\n",
            "neg recall: 0.0132\n",
            "neg F-measure: 0.0256\n",
            "----------------------------\n",
            "    Classification Table\n",
            "----------------------------\n",
            "True Positive (TP) =  1858\n",
            "True Negative (TN) =  4\n",
            "False Positive (FP) =  183\n",
            "False Negative (FN) =  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlmWV_zRNj9P",
        "outputId": "01ad6bd4-98e5-42f4-cfe3-fc9a80e5102b"
      },
      "source": [
        "precision = True_Positive / (True_Positive + False_Positive)\n",
        "recall = True_Positive / float(True_Positive + False_Negative)\n",
        "F1_Score = 2*( (precision*recall) / (precision + recall))\n",
        "acc=nltk.classify.accuracy(model_classifier, test_Dataset)\n",
        "\n",
        "print(\"------------------------\")\n",
        "print(\"  Performance Measures\")\n",
        "print(\"------------------------\")\n",
        "print(\"Accuracy: \", round((acc),3))\n",
        "print(\"Precision: \", round((precision),3))\n",
        "print(\"Recall: \", round((recall),3))\n",
        "print(\"F1 Score: \", round((F1_Score),3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------\n",
            "  Performance Measures\n",
            "------------------------\n",
            "Accuracy:  0.907\n",
            "Precision:  0.91\n",
            "Recall:  0.997\n",
            "F1 Score:  0.952\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}